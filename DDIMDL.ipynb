{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import sqlite3\n",
    "import csv\n",
    "from NLPProcess import NLPProcess\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_num = 65\n",
    "droprate = 0.3\n",
    "vector_size = 572\n",
    "seed = 0\n",
    "CV = 5\n",
    "interaction_num = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity \n",
    "calculates the Jaccard similarity between rows of a matrix.\n",
    "\n",
    "**J(A, B) = |A ∩ B| / |A ∪ B| = |A ∩ B| / |A| + |B| - |A ∩ B|**\n",
    "\n",
    "These features vectors have high dimensions and values of most dimensions are 0, thus we compress features and reduce the sparsity.\n",
    "\n",
    "Instead of using the bit vectors as input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(matrix):\n",
    "    matrix = np.mat(matrix)\n",
    "    numerator = matrix * matrix.T\n",
    "    denominator = (np.ones(np.shape(matrix)) * matrix.T) + \\\n",
    "        (matrix * np.ones(np.shape(matrix.T))) - (matrix * matrix.T)\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.33333333 0.33333333]\n",
      " [0.33333333 1.         0.33333333]\n",
      " [0.33333333 0.33333333 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "# Example input matrix\n",
    "input_matrix = np.array([[1, 0, 1],\n",
    "                         [1, 1, 0],\n",
    "                         [0, 1, 1]])\n",
    "\n",
    "# Calling the Jaccard function\n",
    "jaccard_similarity = Jaccard(input_matrix)\n",
    "\n",
    "# Printing the resulting Jaccard similarity matrix\n",
    "print(jaccard_similarity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector(feature_name, df, vector_size):\n",
    "    # df are the 572 kinds of drugs\n",
    "    all_feature = []\n",
    "    drug_list = np.array(df[feature_name]).tolist()\n",
    "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
    "    for i in drug_list:\n",
    "        all_feature.extend(i.split('|'))\n",
    "    all_feature = list(set(all_feature))\n",
    "\n",
    "    # create taple of uniques features that columns are uniques feature and rows are number of occurance in each drug\n",
    "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
    "    # Consrtuct feature matrices with key of dataframe\n",
    "    df_feature = DataFrame(feature_matrix, columns=all_feature)\n",
    "\n",
    "    for i in range(len(drug_list)):\n",
    "        for each_feature in df[feature_name].iloc[i].split('|'):\n",
    "            df_feature[each_feature].iloc[i] = 1\n",
    "\n",
    "    # Apply Jaccard on the feature\n",
    "    sim_matrix = Jaccard(np.array(df_feature))\n",
    "\n",
    "    # Apply dimensionality reduction Using PCA\n",
    "    # use 572 feature to Create similarity matrix of 572*572\n",
    "    pca = PCA(n_components=vector_size)\n",
    "    pca.fit(sim_matrix)\n",
    "    sim_matrix = pca.transform(sim_matrix)\n",
    "\n",
    "    return sim_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P46098', 'P28223', 'P30556', 'P05412']\n",
      "   P46098  P28223  P30556  P05412\n",
      "0     0.0     0.0     1.0     1.0\n",
      "1     1.0     1.0     0.0     0.0\n",
      "2     1.0     0.0     1.0     0.0\n",
      "[[ 7.07106781e-01 -2.35702260e-01]\n",
      " [-7.07106781e-01 -2.35702260e-01]\n",
      " [ 5.85951041e-17  4.71404521e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Drug A', 'Drug B', 'Drug C'],\n",
    "    'target': ['P30556|P05412', 'P28223|P46098', 'P30556|P46098'],\n",
    "    'feature1': ['A|B|C', 'C|D', 'A|B|D'],\n",
    "    'feature2': ['X|Y', 'Y|Z', 'X'],\n",
    "})\n",
    "\n",
    "# Example feature_name\n",
    "feature_name = 'target'\n",
    "\n",
    "# Example vector_size\n",
    "vector_size = 2\n",
    "\n",
    "# Call the function\n",
    "feature_vector_output = feature_vector(feature_name, df, vector_size)\n",
    "print(feature_vector_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    " Prepares the data by transforming interaction events into numbers, splicing the features, and obtaining feature vectors and labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining feautre vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare(df_drug, feature_list, vector_size, mechanism, action, drugA, drugB):\n",
    "\n",
    "    d_feature = {}\n",
    "    d_label = {}\n",
    "    d_event = [f\"{i} {j}\" for i, j in zip(mechanism, action)]\n",
    "\n",
    "    # sort according to the occurrences of each interaction event\n",
    "    d_event_count = sorted(set(d_event), key=d_event.count, reverse=True)\n",
    "    # make dict of postion of each event according to number  of occurrences\n",
    "    d_label = dict(zip(d_event_count, range(len(d_event_count))))\n",
    "\n",
    "    # Initializes a zero-filled NumPy array vector from n*0 to 0*n\n",
    "    vector = np.zeros(\n",
    "        (len(np.array(df_drug['name']).tolist()), 0), dtype=float)\n",
    "    # Append Features generated features from each feature in drug table\n",
    "    for i in feature_list:\n",
    "        vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))\n",
    "    # create dict that key is drug name and values it's new generated features\n",
    "    for i in range(len(np.array(df_drug['name']).tolist())):\n",
    "        d_feature[np.array(df_drug['name']).tolist()[i]] = vector[i]\n",
    "\n",
    "    # Use the dictionary to obtain feature vector and label\n",
    "    new_feature = []\n",
    "    new_label = []\n",
    "    # Prepare the Feature and label for training and test set\n",
    "    for i in range(len(d_event)):\n",
    "        new_feature.append(\n",
    "            np.hstack((d_feature[drugA[i]], d_feature[drugB[i]])))\n",
    "        new_label.append(d_label[d_event[i]])\n",
    "    new_feature = np.array(new_feature)  # 37264 * 1144\n",
    "    new_label = np.array(new_label)     # 37264 * 1\n",
    "    return (new_feature, new_label, event_num)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    " * **Input Layer**: The model expects input data with a shape of (vector_size * 2,). This means that the input consists of a concatenation of two feature vectors, each of size vector_size.\n",
    "\n",
    "* **Dense Layers**: The input is passed through a series of fully connected dense layers. \n",
    "    * The first dense layer has 512 units and uses the ReLU activation function.\n",
    "    * Batch normalization is applied after each dense layer to normalize the activations and improve training stability. \n",
    "    * Dropout is also applied to prevent overfitting, with the droprate parameter controlling the dropout rate.\n",
    "\n",
    "* **Output Layer**: The final dense layer has event_num units, representing the number of different events or classes in the classification task. The activation function used in this layer is softmax, which produces a probability distribution over the classes.\n",
    "\n",
    "* **Model Compilation**: The model is compiled using the Adam optimizer, which is an adaptive learning rate optimization algorithm. The loss function used is categorical cross-entropy, which is suitable for multi-class classification problems. The accuracy metric is also specified to evaluate the model's performance during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN():\n",
    "    train_input = Input(shape=(vector_size * 2,), name='Inputlayer')\n",
    "    train_in = Dense(512, activation='relu')(train_input)\n",
    "    train_in = BatchNormalization()(train_in)\n",
    "    train_in = Dropout(droprate)(train_in)\n",
    "    train_in = Dense(256, activation='relu')(train_in)\n",
    "    train_in = BatchNormalization()(train_in)\n",
    "    train_in = Dropout(droprate)(train_in)\n",
    "    train_in = Dense(event_num)(train_in)\n",
    "    out = Activation('softmax')(train_in)\n",
    "    model = Model(train_input, out)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(label_matrix, event_num, seed, CV):\n",
    "    index_all_class = np.zeros(len(label_matrix))\n",
    "    for j in range(event_num):\n",
    "        index = np.where(label_matrix == j)\n",
    "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "        k_num = 0\n",
    "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
    "            index_all_class[index[0][test_index]] = k_num\n",
    "            k_num += 1\n",
    "\n",
    "    return index_all_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(clf_type:str, X_train, y_train, X_test, y_test=0):\n",
    "    # Use Selected Model to train the model\n",
    "    if clf_type == 'DDIMDL':\n",
    "        dnn = DNN()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "        dnn.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test),\n",
    "                callbacks=[early_stopping])\n",
    "\n",
    "        # Save model for each flood and feature\n",
    "        filename = f\"model_Flood{k}_Feature{i}.h5\"\n",
    "        dnn.save(filename)\n",
    "        return dnn\n",
    "    elif clf_type == 'RF':\n",
    "        clf = RandomForestClassifier(n_estimators=100)\n",
    "    elif clf_type == 'GBDT':\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif clf_type == 'SVM':\n",
    "        clf = SVC(probability=True)\n",
    "    elif clf_type == 'FM':\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif clf_type == 'KNN':\n",
    "        clf = KNeighborsClassifier(n_neighbors=4)\n",
    "    else:\n",
    "        clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV):\n",
    "    all_eval_type = 11\n",
    "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
    "    each_eval_type = 6\n",
    "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
    "\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    y_score = np.zeros((0, event_num), dtype=float)\n",
    "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
    "    matrix = []\n",
    "    if type(feature_matrix) != list:\n",
    "        matrix.append(feature_matrix)\n",
    "        feature_matrix = matrix\n",
    "\n",
    "    # Iteration Along the Floodes\n",
    "    for k in range(CV):\n",
    "        train_index = np.where(index_all_class != k)\n",
    "        test_index = np.where(index_all_class == k)\n",
    "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
    "\n",
    "        # Itrate Along Selected Features\n",
    "        for i in range(len(feature_matrix)):\n",
    "            # Get Train, Test Dataset\n",
    "            x_train = feature_matrix[i][train_index]\n",
    "            x_test = feature_matrix[i][test_index]\n",
    "            y_train = label_matrix[train_index]\n",
    "            \n",
    "            # one-hot encoding\n",
    "            y_train_one_hot = np.array(y_train)\n",
    "            y_train_one_hot = (np.arange(y_train_one_hot.max() + 1)\n",
    "                               == y_train[:, None]).astype(dtype='float32')\n",
    "            y_test = label_matrix[test_index]\n",
    "            # one-hot encoding\n",
    "            y_test_one_hot = np.array(y_test)\n",
    "            y_test_one_hot = (np.arange(y_test_one_hot.max() + 1)\n",
    "                              == y_test[:, None]).astype(dtype='float32')\n",
    "\n",
    "            # Fit The Selected Model\n",
    "            clf = fit_model(x_train,y_train_one_hot,x_test, y_test_one_hot )\n",
    "            # Predict current flood model\n",
    "            pred += clf.predict_proba(x_test)\n",
    "        \n",
    "        # Get Avrage Predict of all Selected Feature\n",
    "        pred_score = pred / len(feature_matrix)\n",
    "        pred_type = np.argmax(pred_score, axis=1)\n",
    "        \n",
    "        y_true = np.hstack((y_true, y_test))\n",
    "        y_pred = np.hstack((y_pred, pred_type))\n",
    "        y_score = np.row_stack((y_score, pred_score))\n",
    "\n",
    "    return y_pred, y_score, y_true\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred_type, pred_score, y_test, event_num):\n",
    "    all_eval_type = 11\n",
    "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
    "    each_eval_type = 6\n",
    "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
    "    y_one_hot = label_binarize(y_test, np.arange(event_num))\n",
    "    pred_one_hot = label_binarize(pred_type, np.arange(event_num))\n",
    "\n",
    "    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n",
    "\n",
    "    result_all[0] = accuracy_score(y_test, pred_type)\n",
    "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n",
    "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n",
    "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
    "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
    "    result_all[5] = f1_score(y_test, pred_type, average='micro')\n",
    "    result_all[6] = f1_score(y_test, pred_type, average='macro')\n",
    "    result_all[7] = precision_score(y_test, pred_type, average='micro')\n",
    "    result_all[8] = precision_score(y_test, pred_type, average='macro')\n",
    "    result_all[9] = recall_score(y_test, pred_type, average='micro')\n",
    "    result_all[10] = recall_score(y_test, pred_type, average='macro')\n",
    "    for i in range(event_num):\n",
    "        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n",
    "        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
    "                                          average=None)\n",
    "        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
    "                                         average=None)\n",
    "        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
    "                                    average='binary')\n",
    "        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
    "                                           average='binary')\n",
    "        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
    "                                        average='binary')\n",
    "    return [result_all, result_eve] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_aupr_score(y_true, y_score, average=\"macro\"):\n",
    "    def _binary_roc_aupr_score(y_true, y_score):\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    def _average_binary_score(binary_metric, y_true, y_score, average):  # y_true= y_one_hot\n",
    "        if average == \"binary\":\n",
    "            return binary_metric(y_true, y_score)\n",
    "        if average == \"micro\":\n",
    "            y_true = y_true.ravel()\n",
    "            y_score = y_score.ravel()\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape((-1, 1))\n",
    "        if y_score.ndim == 1:\n",
    "            y_score = y_score.reshape((-1, 1))\n",
    "        n_classes = y_score.shape[1]\n",
    "        score = np.zeros((n_classes,))\n",
    "        for c in range(n_classes):\n",
    "            y_true_c = y_true.take([c], axis=1).ravel()\n",
    "            y_score_c = y_score.take([c], axis=1).ravel()\n",
    "            score[c] = binary_metric(y_true_c, y_score_c)\n",
    "        return np.average(score)\n",
    "\n",
    "    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(feature_name, result_type, clf_type, result):\n",
    "    with open(feature_name + '_' + result_type + '_' + clf_type+ '.csv', \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in result:\n",
    "            writer.writerow(i)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(args):\n",
    "    # GET ENTARED DATA\n",
    "    feature_list = args['featureList']\n",
    "    featureName=\"+\".join(feature_list)\n",
    "    clf_list = args['classifier']\n",
    "    nlp_arg = args['NLPProcess']\n",
    "    \n",
    "    # GET REQUIRED DATA FROM DATABASW\n",
    "    conn = sqlite3.connect(\"event.db\")\n",
    "    df_drug = pd.read_sql('select * from drug;', conn)\n",
    "    df_drug.info()\n",
    "    df_event = pd.read_sql('select * from event_number;', conn)\n",
    "    df_interaction = pd.read_sql('select * from event;', conn)\n",
    "   \n",
    "    result_all = {}\n",
    "    result_eve = {}\n",
    "    all_matrix = []\n",
    "    drugList=[]\n",
    "    \n",
    "    for line in open(\"DrugList.txt\",'r'):\n",
    "        drugList.append(line.split()[0])\n",
    "    \n",
    "    ###################### MODEL STEPS #######################\n",
    "    # STEP1: GET DRUG PREPROCESSED FEATURES\n",
    "    # prevoisle saved \n",
    "    if nlp_arg==\"read\":\n",
    "        extraction = pd.read_sql('select * from extraction;', conn)\n",
    "        mechanism = extraction['mechanism']\n",
    "        action = extraction['action']\n",
    "        drugA = extraction['drugA']\n",
    "        drugB = extraction['drugB']\n",
    "    else:\n",
    "        # preprocess features\n",
    "        mechanism,action,drugA,drugB=NLPProcess(drugList,df_interaction)\n",
    "    # STEP2: PREPARATION THE DATA\n",
    "    for feature in feature_list:\n",
    "        print(feature)\n",
    "        new_feature, new_label, event_num = prepare(df_drug, [feature], vector_size, mechanism,action,drugA,drugB)\n",
    "        all_matrix.append(new_feature)\n",
    "    start = time.time()\n",
    "    for clf in clf_list:\n",
    "        # STEP3: TRAIN & PREDICT THE MODEL\n",
    "        pred_type, pred_score, y_test = cross_validation(all_matrix, new_label, clf, event_num, seed, CV)\n",
    "        \n",
    "        # STEP4: EVALUATE THE MODEL\n",
    "        all_result, each_result = evaluate(pred_type, pred_score, y_test, event_num)\n",
    "        save_result(featureName, 'all', clf, all_result)\n",
    "        save_result(featureName, 'each', clf, each_result)\n",
    "        result_all[clf] = all_result\n",
    "        result_eve[clf] = each_result\n",
    "    print(\"time used:\", (time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"-f\",\"--featureList\",default=[\"smile\",\"target\",\"enzyme\"],help=\"features to use\",nargs=\"+\")\n",
    "    parser.add_argument(\"-c\",\"--classifier\",choices=[\"DDIMDL\",\"RF\",\"KNN\",\"LR\"],default=[\"DDIMDL\"],help=\"classifiers to use\",nargs=\"+\")\n",
    "    parser.add_argument(\"-p\",\"--NLPProcess\",choices=[\"read\",\"process\"],default=\"read\",help=\"Read the NLP extraction result directly or process the events again\")\n",
    "    args=vars(parser.parse_args())\n",
    "    print(\"mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\")\n",
    "    print(args)\n",
    "    main(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abd+'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = [\"osama\", \"mohamed\", \"abd\"]\n",
    "\n",
    "for feature in feature_list:\n",
    "    set_name = feature + '+'\n",
    "set_name = set_name[:]\n",
    "set_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"+\".join(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'osama+mohamed+abd'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "new_model = h5py.File(\"model_fold_0.h5\", 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = np.load('Drug_Smile_Features.npz')\n",
    "\n",
    "# Convert the loaded dictionary to a regular Python dictionary\n",
    "d_feature = dict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.hstack((d_feature[\"Diltiazem\"], d_feature[\"Abemaciclib\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1144)\n"
     ]
    }
   ],
   "source": [
    "test = test.reshape((1, 1144))\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'File' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eng Osama Mo\\OneDrive\\Desktop\\final DDI\\DDIMDL.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eng%20Osama%20Mo/OneDrive/Desktop/final%20DDI/DDIMDL.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(new_model\u001b[39m.\u001b[39;49mpredict(test), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eng%20Osama%20Mo/OneDrive/Desktop/final%20DDI/DDIMDL.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'File' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "x = np.argmax(new_model.predict(test), axis=1)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"event.db\")\n",
    "df_drug = pd.read_sql('select * from event_number;', conn)\n",
    "#\n",
    "x = dict(zip([str(i) for i in range(len(df_drug[\"event\"]))], df_drug[\"event\"]))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f'Event_Number.npz', **x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The metabolism of name can be decreased when combined with name.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event = x[str(np.argmax(new_model.predict(test), axis=1)[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__name__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:57884\n",
      " * Running on http://192.168.1.3:57884\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glucosamine\n",
      "Anagrelide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-04 23:47:49,381] ERROR in app: Exception on /user [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"c:\\Users\\Eng Osama Mo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\Eng Osama Mo\\AppData\\Local\\Temp\\ipykernel_11920\\3648745965.py\", line 21, in request_page\n",
      "    print(predict_event(drug1, drug2))\n",
      "  File \"C:\\Users\\Eng Osama Mo\\AppData\\Local\\Temp\\ipykernel_11920\\1596845647.py\", line 23, in predict_event\n",
      "    event = d_event[str(np.argmax(new_model.predict(test), axis=1)[0])]\n",
      "NameError: name 'test' is not defined\n",
      "192.168.1.2 - - [04/Jun/2023 23:47:49] \"GET /user?user=Glucosamine|Anagrelide HTTP/1.1\" 500 -\n",
      "192.168.1.2 - - [04/Jun/2023 23:47:49] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def load_npz(file):\n",
    "    loaded_dict = np.load(file)\n",
    "    # Convert the loaded dictionary to a regular Python dictionary\n",
    "    dict_file = dict(loaded_dict)\n",
    "    return dict_file\n",
    "\n",
    "\n",
    "d_feature = load_npz(\"Drug_Smile_Features.npz\")\n",
    "d_event = load_npz(\"Event_Number.npz\")\n",
    "\n",
    "\n",
    "def predict_event(drug1: str, drug2: str):\n",
    "    drugs_feature = np.hstack((d_feature[drug1], d_feature[drug2]))\n",
    "    drugs_feature = drugs_feature.reshape((1,1144))\n",
    "\n",
    "    new_model = load_model(\"model_fold_0.h5\")\n",
    "    event = d_event[str(np.argmax(new_model.predict(drugs_feature), axis=1)[0])]\n",
    "    event = str(event)\n",
    "    # Replace the first occurrence of \"name\" with drug1\n",
    "    new_event = event.replace(\"name\", drug1, 1)\n",
    "    # Replace the second occurrence of \"name\" with drug2\n",
    "    new_event = new_event.replace(\"name\", drug2, 1)\n",
    "    return new_event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1144)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000262616B0E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "Glucosamine may increase the antiplatelet activities of Glucosamine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_npz(file):\n",
    "    loaded_dict = np.load(file)\n",
    "    # Convert the loaded dictionary to a regular Python dictionary\n",
    "    dict_file = dict(loaded_dict)\n",
    "    return dict_file\n",
    "\n",
    "\n",
    "d_feature = load_npz(\"Drug_Smile_Features.npz\")\n",
    "d_event = load_npz(\"Event_Number.npz\")\n",
    "\n",
    "drugs_feature = np.hstack((d_feature[drug1], d_feature[drug2]))\n",
    "drugs_feature = drugs_feature.reshape((1,1144))\n",
    "\n",
    "new_model = load_model(\"model_fold_0.h5\")\n",
    "event = d_event[str(np.argmax(new_model.predict(drugs_feature), axis=1)[0])]\n",
    "event = str(event)\n",
    "# Replace the first occurrence of \"name\" with drug1\n",
    "new_event = event.replace(\"name\", drug1, 1)\n",
    "# Replace the second occurrence of \"name\" with drug2\n",
    "new_event = new_event.replace(\"name\", drug2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__name__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:58286\n",
      " * Running on http://192.168.1.3:58286\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glucosamine\n",
      "Anagrelide\n",
      "1/1 [==============================] - 0s 304ms/step\n",
      "The metabolism of Glucosamine can be decreased when combined with Anagrelide.\n",
      "1/1 [==============================] - 0s 278ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Jun/2023 00:24:42] \"GET /user?user=Glucosamine-Anagrelide HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "import time\n",
    "app = Flask('__name__')\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=['GET'])\n",
    "def home_page():\n",
    "    data_set = {\"page\": \"Home\", \"Message\": \"Succes\", \"Timetamp\": time.time()}\n",
    "    json_dump = json.dumps(data_set)\n",
    "\n",
    "    return json_dump\n",
    "\n",
    "\n",
    "@app.route(\"/user\", methods=['GET'])\n",
    "def request_page():\n",
    "    user_query = str(request.args.get(\"user\"))  # /user/?user=\n",
    "    drug1, drug2 = user_query.split(\"-\")\n",
    "    print(drug1)\n",
    "    print(drug2)\n",
    "    print(predict_event(drug1, drug2))\n",
    "    data_set = {\"Event\": predict_event(drug1, drug2)}\n",
    "    json_dump = json.dumps(data_set)\n",
    "\n",
    "    return json_dump\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=0000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a9cb064a66d6bf474623f10a32dcaa1e6bd1eee0b2ba988d2ac7a1661d857d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
